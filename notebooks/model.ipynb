{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Toby Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Loading the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"../data/preprocessed/Unsampled_train_dataset.csv\")\n",
    "train_dataset_undersampled = pd.read_csv(\"../data/preprocessed/Undersampled_train_dataset.csv\")\n",
    "train_dataset_oversampled = pd.read_csv(\"../data/preprocessed/Oversampled_train_dataset.csv\")\n",
    "train_dataset_smote = pd.read_csv(\"../data/preprocessed/SMOTE_train_dataset.csv\")\n",
    "\n",
    "train_datasets_dict = {\"Unsampled\": train_dataset, \"Undersampled\": train_dataset_undersampled, \"Oversampled\": train_dataset_oversampled, \"SMOTE\": train_dataset_smote}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Implementing Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def train_model(train_features, train_labels, val_features, val_labels, lr = 0.05, dp = 0.1, units = 64, batch_size = 32, epochs = 20):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer and first hidden\n",
    "    model.add(Dense(units, activation = \"relu\", input_dim = train_features.shape[1]))\n",
    "    model.add(Dropout(dp))\n",
    "    \n",
    "    # Additional hidden layer\n",
    "    model.add(Dense(units, activation = \"relu\"))\n",
    "    model.add(Dropout(dp))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation = \"sigmoid\"))\n",
    "    model.compile(optimizer = Adam(lr = lr), loss=\"binary_crossentropy\", metrics = [\"acc\"])\n",
    "    \n",
    "    return (model, model.fit(train_features, train_labels, epochs = epochs, batch_size = batch_size, validation_data = (val_features, val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_path = \"./tuning/tuning_results.csv\"\n",
    "tuning_results = pd.read_csv(tuning_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Units</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>F1 Avg</th>\n",
       "      <th>F1 Std</th>\n",
       "      <th>Recall Avg</th>\n",
       "      <th>Recall Std</th>\n",
       "      <th>Precision Avg</th>\n",
       "      <th>Precision Std</th>\n",
       "      <th>Accuracy Avg</th>\n",
       "      <th>Accuracy Std</th>\n",
       "      <th>Iterations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.496</td>\n",
       "      <td>4.052456e-01</td>\n",
       "      <td>0.539130</td>\n",
       "      <td>0.440484</td>\n",
       "      <td>0.999013</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.078009</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.960795</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.614738</td>\n",
       "      <td>0.098421</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.472839</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.997843</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.511679</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.998280</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.419913</td>\n",
       "      <td>0.336</td>\n",
       "      <td>4.115143e-01</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.428661</td>\n",
       "      <td>0.998745</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.064499</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.949559</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.645587</td>\n",
       "      <td>0.061959</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.505760</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.998210</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.710582</td>\n",
       "      <td>0.044788</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.583925</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>0.998675</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.338095</td>\n",
       "      <td>0.414149</td>\n",
       "      <td>0.328</td>\n",
       "      <td>4.019154e-01</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.427333</td>\n",
       "      <td>0.998731</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.064615</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.944</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.950786</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.653978</td>\n",
       "      <td>0.049611</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.512938</td>\n",
       "      <td>0.061215</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.696742</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.565017</td>\n",
       "      <td>0.042727</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.680952</td>\n",
       "      <td>0.340601</td>\n",
       "      <td>0.664</td>\n",
       "      <td>3.323612e-01</td>\n",
       "      <td>0.698913</td>\n",
       "      <td>0.349463</td>\n",
       "      <td>0.999239</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.059307</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.030639</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.947825</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.701360</td>\n",
       "      <td>0.066504</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.574497</td>\n",
       "      <td>0.085008</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.742228</td>\n",
       "      <td>0.034264</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.628337</td>\n",
       "      <td>0.054662</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>0.832</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.881522</td>\n",
       "      <td>0.015901</td>\n",
       "      <td>0.999507</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.072299</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.529822e-02</td>\n",
       "      <td>0.037567</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.956509</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.737182</td>\n",
       "      <td>0.033156</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>0.998858</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.761806</td>\n",
       "      <td>0.012809</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.650253</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.493224</td>\n",
       "      <td>0.403842</td>\n",
       "      <td>0.464</td>\n",
       "      <td>3.808202e-01</td>\n",
       "      <td>0.526765</td>\n",
       "      <td>0.430536</td>\n",
       "      <td>0.998943</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.077722</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.960302</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.632591</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.880</td>\n",
       "      <td>2.529822e-02</td>\n",
       "      <td>0.494175</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.998196</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.725293</td>\n",
       "      <td>0.032295</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.606358</td>\n",
       "      <td>0.040140</td>\n",
       "      <td>0.998788</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.822265</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>0.760</td>\n",
       "      <td>1.011929e-01</td>\n",
       "      <td>0.913302</td>\n",
       "      <td>0.053547</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.055810</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.968</td>\n",
       "      <td>3.919184e-02</td>\n",
       "      <td>0.028744</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.941312</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.761677</td>\n",
       "      <td>0.041540</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.655171</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.767719</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.872</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.688026</td>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.839062</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>0.776</td>\n",
       "      <td>4.800000e-02</td>\n",
       "      <td>0.915719</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.063186</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.032697</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.947332</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.835901</td>\n",
       "      <td>0.043442</td>\n",
       "      <td>0.888</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.791503</td>\n",
       "      <td>0.062360</td>\n",
       "      <td>0.999380</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.764508</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.904</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.663702</td>\n",
       "      <td>0.047992</td>\n",
       "      <td>0.999013</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.858341</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.824</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.898047</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.960</td>\n",
       "      <td>3.577709e-02</td>\n",
       "      <td>0.037163</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.955635</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.756850</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.085130</td>\n",
       "      <td>0.998929</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.794005</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.904</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.708809</td>\n",
       "      <td>0.051884</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.853867</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.800</td>\n",
       "      <td>5.656854e-02</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.104473</td>\n",
       "      <td>0.009737</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.055167</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.969747</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.855007</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.896</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.817725</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.825400</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.872</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>0.023058</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.867076</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>0.816</td>\n",
       "      <td>6.499231e-02</td>\n",
       "      <td>0.930007</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.529822e-02</td>\n",
       "      <td>0.052454</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.968873</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.789848</td>\n",
       "      <td>0.050029</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.700934</td>\n",
       "      <td>0.076833</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.825312</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>0.888</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.771883</td>\n",
       "      <td>0.026160</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.784</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.104051</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.952</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.055072</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.970748</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.796174</td>\n",
       "      <td>0.016187</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.702119</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.795802</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>0.888</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.722007</td>\n",
       "      <td>0.040388</td>\n",
       "      <td>0.999196</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>0.744</td>\n",
       "      <td>5.425864e-02</td>\n",
       "      <td>0.943337</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.135940</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.073520</td>\n",
       "      <td>0.011272</td>\n",
       "      <td>0.978910</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.788711</td>\n",
       "      <td>0.028393</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.701665</td>\n",
       "      <td>0.048932</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.798830</td>\n",
       "      <td>0.029344</td>\n",
       "      <td>0.872</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.739554</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.848659</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.784</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.925108</td>\n",
       "      <td>0.046817</td>\n",
       "      <td>0.999507</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.118274</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.063165</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.975174</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.051752</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.732836</td>\n",
       "      <td>0.088523</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.784335</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.872</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.999154</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.847587</td>\n",
       "      <td>0.054736</td>\n",
       "      <td>0.776</td>\n",
       "      <td>9.666437e-02</td>\n",
       "      <td>0.946894</td>\n",
       "      <td>0.043532</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.095007</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.968</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.049994</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.967083</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.787298</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.904</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.702016</td>\n",
       "      <td>0.070453</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.792552</td>\n",
       "      <td>0.039712</td>\n",
       "      <td>0.880</td>\n",
       "      <td>4.381780e-02</td>\n",
       "      <td>0.724465</td>\n",
       "      <td>0.064169</td>\n",
       "      <td>0.999182</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dataset  Learning Rate  Dropout  Units  Batch Size    F1 Avg  \\\n",
       "0      Unsampled         0.0100      0.1     64          32  0.516667   \n",
       "1   Undersampled         0.0100      0.1     64          32  0.078009   \n",
       "2    Oversampled         0.0100      0.1     64          32  0.614738   \n",
       "3          SMOTE         0.0100      0.1     64          32  0.656100   \n",
       "4      Unsampled         0.0100      0.2     64          32  0.342857   \n",
       "5   Undersampled         0.0100      0.2     64          32  0.064499   \n",
       "6    Oversampled         0.0100      0.2     64          32  0.645587   \n",
       "7          SMOTE         0.0100      0.2     64          32  0.710582   \n",
       "8      Unsampled         0.0100      0.3     64          32  0.338095   \n",
       "9   Undersampled         0.0100      0.3     64          32  0.064615   \n",
       "10   Oversampled         0.0100      0.3     64          32  0.653978   \n",
       "11         SMOTE         0.0100      0.3     64          32  0.696742   \n",
       "12     Unsampled         0.0100      0.1     32          32  0.680952   \n",
       "13  Undersampled         0.0100      0.1     32          32  0.059307   \n",
       "14   Oversampled         0.0100      0.1     32          32  0.701360   \n",
       "15         SMOTE         0.0100      0.1     32          32  0.742228   \n",
       "16     Unsampled         0.0050      0.2     32          32  0.855952   \n",
       "17  Undersampled         0.0050      0.2     32          32  0.072299   \n",
       "18   Oversampled         0.0050      0.2     32          32  0.737182   \n",
       "19         SMOTE         0.0050      0.2     32          32  0.761806   \n",
       "20     Unsampled         0.0100      0.2     32          32  0.493224   \n",
       "21  Undersampled         0.0100      0.2     32          32  0.077722   \n",
       "22   Oversampled         0.0100      0.2     32          32  0.632591   \n",
       "23         SMOTE         0.0100      0.2     32          32  0.725293   \n",
       "24     Unsampled         0.0100      0.0     32          32  0.822265   \n",
       "25  Undersampled         0.0100      0.0     32          32  0.055810   \n",
       "26   Oversampled         0.0100      0.0     32          32  0.761677   \n",
       "27         SMOTE         0.0100      0.0     32          32  0.767719   \n",
       "28     Unsampled         0.0050      0.0     32          32  0.839062   \n",
       "29  Undersampled         0.0050      0.0     32          32  0.063186   \n",
       "30   Oversampled         0.0050      0.0     32          32  0.835901   \n",
       "31         SMOTE         0.0050      0.0     32          32  0.764508   \n",
       "32     Unsampled         0.0050      0.1     32          32  0.858341   \n",
       "33  Undersampled         0.0050      0.1     32          32  0.071521   \n",
       "34   Oversampled         0.0050      0.1     32          32  0.756850   \n",
       "35         SMOTE         0.0050      0.1     32          32  0.794005   \n",
       "36     Unsampled         0.0010      0.0     32          32  0.853867   \n",
       "37  Undersampled         0.0010      0.0     32          32  0.104473   \n",
       "38   Oversampled         0.0010      0.0     32          32  0.855007   \n",
       "39         SMOTE         0.0010      0.0     32          32  0.825400   \n",
       "40     Unsampled         0.0010      0.1     32          32  0.867076   \n",
       "41  Undersampled         0.0010      0.1     32          32  0.099392   \n",
       "42   Oversampled         0.0010      0.1     32          32  0.789848   \n",
       "43         SMOTE         0.0010      0.1     32          32  0.825312   \n",
       "44     Unsampled         0.0010      0.2     32          32  0.841121   \n",
       "45  Undersampled         0.0010      0.2     32          32  0.104051   \n",
       "46   Oversampled         0.0010      0.2     32          32  0.796174   \n",
       "47         SMOTE         0.0010      0.2     32          32  0.795802   \n",
       "48     Unsampled         0.0005      0.2     32          32  0.829757   \n",
       "49  Undersampled         0.0005      0.2     32          32  0.135940   \n",
       "50   Oversampled         0.0005      0.2     32          32  0.788711   \n",
       "51         SMOTE         0.0005      0.2     32          32  0.798830   \n",
       "52     Unsampled         0.0005      0.1     32          32  0.848659   \n",
       "53  Undersampled         0.0005      0.1     32          32  0.118274   \n",
       "54   Oversampled         0.0005      0.1     32          32  0.809278   \n",
       "55         SMOTE         0.0005      0.1     32          32  0.784335   \n",
       "56     Unsampled         0.0010      0.1     32          16  0.847587   \n",
       "57  Undersampled         0.0010      0.1     32          16  0.095007   \n",
       "58   Oversampled         0.0010      0.1     32          16  0.787298   \n",
       "59         SMOTE         0.0010      0.1     32          16  0.792552   \n",
       "\n",
       "      F1 Std  Recall Avg    Recall Std  Precision Avg  Precision Std  \\\n",
       "0   0.422131       0.496  4.052456e-01       0.539130       0.440484   \n",
       "1   0.009770       0.928  1.600000e-02       0.040738       0.005308   \n",
       "2   0.098421       0.912  1.600000e-02       0.472839       0.111801   \n",
       "3   0.042169       0.920  1.110223e-16       0.511679       0.051370   \n",
       "4   0.419913       0.336  4.115143e-01       0.350000       0.428661   \n",
       "5   0.012134       0.944  1.959592e-02       0.033435       0.006487   \n",
       "6   0.061959       0.904  1.959592e-02       0.505760       0.075151   \n",
       "7   0.044788       0.912  1.600000e-02       0.583925       0.056182   \n",
       "8   0.414149       0.328  4.019154e-01       0.348913       0.427333   \n",
       "9   0.009789       0.944  3.200000e-02       0.033478       0.005242   \n",
       "10  0.049611       0.912  1.600000e-02       0.512938       0.061215   \n",
       "11  0.033807       0.912  1.600000e-02       0.565017       0.042727   \n",
       "12  0.340601       0.664  3.323612e-01       0.698913       0.349463   \n",
       "13  0.004300       0.928  1.600000e-02       0.030639       0.002307   \n",
       "14  0.066504       0.912  1.600000e-02       0.574497       0.085008   \n",
       "15  0.034264       0.912  1.600000e-02       0.628337       0.054662   \n",
       "16  0.013257       0.832  1.600000e-02       0.881522       0.015901   \n",
       "17  0.003442       0.960  2.529822e-02       0.037567       0.001841   \n",
       "18  0.033156       0.904  1.959592e-02       0.623206       0.041547   \n",
       "19  0.012809       0.920  1.110223e-16       0.650253       0.018608   \n",
       "20  0.403842       0.464  3.808202e-01       0.526765       0.430536   \n",
       "21  0.005147       0.944  1.959592e-02       0.040541       0.002829   \n",
       "22  0.022157       0.880  2.529822e-02       0.494175       0.023940   \n",
       "23  0.032295       0.904  1.959592e-02       0.606358       0.040140   \n",
       "24  0.054001       0.760  1.011929e-01       0.913302       0.053547   \n",
       "25  0.007100       0.968  3.919184e-02       0.028744       0.003732   \n",
       "26  0.041540       0.912  1.600000e-02       0.655171       0.053576   \n",
       "27  0.056569       0.872  2.993326e-02       0.688026       0.073092   \n",
       "28  0.026614       0.776  4.800000e-02       0.915719       0.016394   \n",
       "29  0.010742       0.968  1.600000e-02       0.032697       0.005726   \n",
       "30  0.043442       0.888  2.993326e-02       0.791503       0.062360   \n",
       "31  0.036624       0.904  3.200000e-02       0.663702       0.047992   \n",
       "32  0.014841       0.824  3.200000e-02       0.898047       0.036887   \n",
       "33  0.006630       0.960  3.577709e-02       0.037163       0.003612   \n",
       "34  0.060802       0.920  1.110223e-16       0.647619       0.085130   \n",
       "35  0.042254       0.904  3.200000e-02       0.708809       0.051884   \n",
       "36  0.021458       0.800  5.656854e-02       0.920950       0.035547   \n",
       "37  0.009737       0.992  1.600000e-02       0.055167       0.005418   \n",
       "38  0.019059       0.896  1.959592e-02       0.817725       0.021071   \n",
       "39  0.016656       0.872  4.664762e-02       0.785689       0.023058   \n",
       "40  0.038706       0.816  6.499231e-02       0.930007       0.044900   \n",
       "41  0.012528       0.960  2.529822e-02       0.052454       0.006989   \n",
       "42  0.050029       0.912  1.600000e-02       0.700934       0.076833   \n",
       "43  0.017780       0.888  2.993326e-02       0.771883       0.026160   \n",
       "44  0.016977       0.784  3.200000e-02       0.909545       0.036261   \n",
       "45  0.011450       0.952  2.993326e-02       0.055072       0.006395   \n",
       "46  0.016187       0.920  1.110223e-16       0.702119       0.025315   \n",
       "47  0.036520       0.888  4.664762e-02       0.722007       0.040388   \n",
       "48  0.035797       0.744  5.425864e-02       0.943337       0.053493   \n",
       "49  0.019270       0.920  1.110223e-16       0.073520       0.011272   \n",
       "50  0.028393       0.904  1.959592e-02       0.701665       0.048932   \n",
       "51  0.029344       0.872  4.664762e-02       0.739554       0.042900   \n",
       "52  0.037982       0.784  3.200000e-02       0.925108       0.046817   \n",
       "53  0.011065       0.936  1.959592e-02       0.063165       0.006301   \n",
       "54  0.051752       0.912  1.600000e-02       0.732836       0.088523   \n",
       "55  0.036037       0.872  4.664762e-02       0.713900       0.040837   \n",
       "56  0.054736       0.776  9.666437e-02       0.946894       0.043532   \n",
       "57  0.010841       0.968  2.993326e-02       0.049994       0.006048   \n",
       "58  0.040833       0.904  3.200000e-02       0.702016       0.070453   \n",
       "59  0.039712       0.880  4.381780e-02       0.724465       0.064169   \n",
       "\n",
       "    Accuracy Avg  Accuracy Std  Iterations  \n",
       "0       0.999013      0.000635           5  \n",
       "1       0.960795      0.004513           5  \n",
       "2       0.997843      0.000893           5  \n",
       "3       0.998280      0.000317           5  \n",
       "4       0.998745      0.000622           5  \n",
       "5       0.949559      0.011972           5  \n",
       "6       0.998210      0.000456           5  \n",
       "7       0.998675      0.000283           5  \n",
       "8       0.998731      0.000605           5  \n",
       "9       0.950786      0.007156           5  \n",
       "10      0.998266      0.000415           5  \n",
       "11      0.998590      0.000232           5  \n",
       "12      0.999239      0.000501           5  \n",
       "13      0.947825      0.004191           5  \n",
       "14      0.998590      0.000452           5  \n",
       "15      0.998872      0.000214           5  \n",
       "16      0.999507      0.000045           5  \n",
       "17      0.956509      0.001917           5  \n",
       "18      0.998858      0.000181           5  \n",
       "19      0.998985      0.000072           5  \n",
       "20      0.998943      0.000583           5  \n",
       "21      0.960302      0.003327           5  \n",
       "22      0.998196      0.000145           5  \n",
       "23      0.998788      0.000175           5  \n",
       "24      0.999436      0.000118           5  \n",
       "25      0.941312      0.007919           5  \n",
       "26      0.998985      0.000221           5  \n",
       "27      0.999055      0.000288           5  \n",
       "28      0.999478      0.000072           5  \n",
       "29      0.947332      0.012448           5  \n",
       "30      0.999380      0.000186           5  \n",
       "31      0.999013      0.000178           5  \n",
       "32      0.999521      0.000053           5  \n",
       "33      0.955635      0.004936           5  \n",
       "34      0.998929      0.000371           5  \n",
       "35      0.999168      0.000191           5  \n",
       "36      0.999521      0.000053           5  \n",
       "37      0.969747      0.003124           5  \n",
       "38      0.999464      0.000072           5  \n",
       "39      0.999352      0.000053           5  \n",
       "40      0.999563      0.000121           5  \n",
       "41      0.968873      0.003797           5  \n",
       "42      0.999126      0.000284           5  \n",
       "43      0.999337      0.000072           5  \n",
       "44      0.999478      0.000056           5  \n",
       "45      0.970748      0.003463           5  \n",
       "46      0.999168      0.000082           5  \n",
       "47      0.999196      0.000145           5  \n",
       "48      0.999464      0.000105           5  \n",
       "49      0.978910      0.003425           5  \n",
       "50      0.999140      0.000157           5  \n",
       "51      0.999225      0.000126           5  \n",
       "52      0.999507      0.000126           5  \n",
       "53      0.975174      0.002551           5  \n",
       "54      0.999225      0.000260           5  \n",
       "55      0.999154      0.000148           5  \n",
       "56      0.999521      0.000137           5  \n",
       "57      0.967083      0.003828           5  \n",
       "58      0.999126      0.000230           5  \n",
       "59      0.999182      0.000176           5  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "\n",
    "def compute_stats(stat_lst):\n",
    "    return (np.average(stat_lst), np.std(stat_lst))\n",
    "\n",
    "def save_plot(hist, key, lr, dp, units, batch_size):\n",
    "    title = \"{}-lr{}-dp{}-units{}-batchsize{}\".format(key, lr, dp, units, batch_size)\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    plt.title(title)\n",
    "    plt.plot(hist.history[\"loss\"], label = \"Training loss\")\n",
    "    plt.plot(hist.history[\"val_loss\"], label = \"Validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./tuning/plots/{}.png\".format(title))\n",
    "    plt.close(fig)\n",
    "\n",
    "def tune_hyperparameters(key, lr, dp, units, batch_size, iterations):\n",
    "        prec_lst, rec_lst, f1_lst, acc_lst = [], [], [], []\n",
    "\n",
    "        for i in range(iterations):\n",
    "            print(\"Dataset: {}, Iteration: {}\".format(key, i + 1))\n",
    "\n",
    "            model, hist = train_model(train_features_dict[key].values, train_labels_dict[key].values, val_features.values, val_labels.values, lr = lr, dp = dp, units = units, batch_size = batch_size)\n",
    "            predictions = model.predict_classes(test_features)\n",
    "            \n",
    "            # Plot loss\n",
    "            save_plot(hist, key, lr, dp, units, batch_size)\n",
    "\n",
    "            # Append evaluation scores to lists\n",
    "            f1_lst.append(f1_score(test_labels, predictions))\n",
    "            rec_lst.append(recall_score(test_labels, predictions))\n",
    "            prec_lst.append(precision_score(test_labels, predictions))\n",
    "            acc_lst.append(accuracy_score(test_labels, predictions))\n",
    "\n",
    "        # Compute stats\n",
    "        f1_avg, f1_std = compute_stats(f1_lst)\n",
    "        rec_avg, rec_std = compute_stats(rec_lst)\n",
    "        prec_avg, prec_std = compute_stats(prec_lst)\n",
    "        acc_avg, acc_std = compute_stats(acc_lst)\n",
    "\n",
    "        return pd.DataFrame({\"Dataset\": [key], \"Learning Rate\": [lr], \"Dropout\": [dp], \"Units\": [units], \"Batch Size\":[batch_size], \"F1 Avg\": [f1_avg], \"F1 Std\": [f1_std], \"Recall Avg\": [rec_avg], \"Recall Std\": [rec_std], \"Precision Avg\": [prec_avg], \"Precision Std\": [prec_std], \"Accuracy Avg\": [acc_avg], \"Accuracy Std\": [acc_std], \"Iterations\": [iterations]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Unsampled, Iteration: 1\n",
      "Train on 261452 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "261452/261452 [==============================] - 28s 107us/step - loss: 0.0072 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "261452/261452 [==============================] - 23s 88us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "261452/261452 [==============================] - 23s 88us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "261452/261452 [==============================] - 23s 88us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Unsampled, Iteration: 2\n",
      "Train on 261452 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "261452/261452 [==============================] - 26s 100us/step - loss: 0.0058 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Unsampled, Iteration: 3\n",
      "Train on 261452 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "261452/261452 [==============================] - 27s 101us/step - loss: 0.0068 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "261452/261452 [==============================] - 24s 90us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "261452/261452 [==============================] - 23s 89us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "261452/261452 [==============================] - 23s 90us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Unsampled, Iteration: 4\n",
      "Train on 261452 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "261452/261452 [==============================] - 27s 104us/step - loss: 0.0057 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Unsampled, Iteration: 5\n",
      "Train on 261452 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "261452/261452 [==============================] - 27s 102us/step - loss: 0.0057 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0028 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "261452/261452 [==============================] - 24s 92us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "261452/261452 [==============================] - 24s 91us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "261452/261452 [==============================] - 24s 93us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Undersampled, Iteration: 1\n",
      "Train on 868 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "868/868 [==============================] - 3s 4ms/step - loss: 0.4168 - acc: 0.8272 - val_loss: 15.8957 - val_acc: 0.0023\n",
      "Epoch 2/20\n",
      "868/868 [==============================] - 0s 519us/step - loss: 0.2629 - acc: 0.9182 - val_loss: 15.8860 - val_acc: 0.0023\n",
      "Epoch 3/20\n",
      "868/868 [==============================] - 0s 524us/step - loss: 0.2121 - acc: 0.9274 - val_loss: 0.0288 - val_acc: 0.9981\n",
      "Epoch 4/20\n",
      "868/868 [==============================] - 0s 518us/step - loss: 0.1874 - acc: 0.9355 - val_loss: 0.0387 - val_acc: 0.9968\n",
      "Epoch 5/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.1626 - acc: 0.9355 - val_loss: 0.0359 - val_acc: 0.9974\n",
      "Epoch 6/20\n",
      "868/868 [==============================] - 0s 516us/step - loss: 0.1617 - acc: 0.9401 - val_loss: 0.0411 - val_acc: 0.9970\n",
      "Epoch 7/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.1486 - acc: 0.9424 - val_loss: 0.0350 - val_acc: 0.9977\n",
      "Epoch 8/20\n",
      "868/868 [==============================] - 0s 519us/step - loss: 0.1490 - acc: 0.9435 - val_loss: 0.0347 - val_acc: 0.9977\n",
      "Epoch 9/20\n",
      "868/868 [==============================] - 0s 533us/step - loss: 0.1381 - acc: 0.9470 - val_loss: 0.0377 - val_acc: 0.9974\n",
      "Epoch 10/20\n",
      "868/868 [==============================] - 0s 522us/step - loss: 0.1330 - acc: 0.9482 - val_loss: 0.0355 - val_acc: 0.9977\n",
      "Epoch 11/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.1289 - acc: 0.9505 - val_loss: 0.0354 - val_acc: 0.9977\n",
      "Epoch 12/20\n",
      "868/868 [==============================] - 0s 516us/step - loss: 0.1232 - acc: 0.9516 - val_loss: 0.0354 - val_acc: 0.9977\n",
      "Epoch 13/20\n",
      "868/868 [==============================] - 0s 557us/step - loss: 0.1338 - acc: 0.9493 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868/868 [==============================] - 0s 525us/step - loss: 0.1202 - acc: 0.9528 - val_loss: 0.0319 - val_acc: 0.9980\n",
      "Epoch 15/20\n",
      "868/868 [==============================] - 0s 519us/step - loss: 0.1084 - acc: 0.9562 - val_loss: 0.0319 - val_acc: 0.9980\n",
      "Epoch 16/20\n",
      "868/868 [==============================] - 0s 529us/step - loss: 0.1128 - acc: 0.9528 - val_loss: 0.0339 - val_acc: 0.9978\n",
      "Epoch 17/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.0989 - acc: 0.9574 - val_loss: 0.0319 - val_acc: 0.9980\n",
      "Epoch 18/20\n",
      "868/868 [==============================] - 0s 529us/step - loss: 0.1053 - acc: 0.9574 - val_loss: 0.0350 - val_acc: 0.9978\n",
      "Epoch 19/20\n",
      "868/868 [==============================] - 0s 525us/step - loss: 0.0997 - acc: 0.9597 - val_loss: 0.0319 - val_acc: 0.9979\n",
      "Epoch 20/20\n",
      "868/868 [==============================] - 0s 528us/step - loss: 0.0939 - acc: 0.9597 - val_loss: 0.0349 - val_acc: 0.9978\n",
      "Dataset: Undersampled, Iteration: 2\n",
      "Train on 868 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "868/868 [==============================] - 3s 4ms/step - loss: 0.5155 - acc: 0.7488 - val_loss: 0.0299 - val_acc: 0.9981\n",
      "Epoch 2/20\n",
      "868/868 [==============================] - 0s 531us/step - loss: 0.3028 - acc: 0.9090 - val_loss: 0.0281 - val_acc: 0.9981\n",
      "Epoch 3/20\n",
      "868/868 [==============================] - 0s 537us/step - loss: 0.2412 - acc: 0.9286 - val_loss: 0.0299 - val_acc: 0.9981\n",
      "Epoch 4/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.2139 - acc: 0.9320 - val_loss: 0.0300 - val_acc: 0.9980\n",
      "Epoch 5/20\n",
      "868/868 [==============================] - 0s 574us/step - loss: 0.1887 - acc: 0.9320 - val_loss: 0.0300 - val_acc: 0.9980\n",
      "Epoch 6/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.1741 - acc: 0.9343 - val_loss: 0.0312 - val_acc: 0.9980\n",
      "Epoch 7/20\n",
      "868/868 [==============================] - 0s 536us/step - loss: 0.1690 - acc: 0.9320 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 8/20\n",
      "868/868 [==============================] - 0s 575us/step - loss: 0.1504 - acc: 0.9482 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 9/20\n",
      "868/868 [==============================] - 0s 535us/step - loss: 0.1483 - acc: 0.9435 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 10/20\n",
      "868/868 [==============================] - 0s 524us/step - loss: 0.1422 - acc: 0.9412 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 11/20\n",
      "868/868 [==============================] - 0s 528us/step - loss: 0.1337 - acc: 0.9447 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 12/20\n",
      "868/868 [==============================] - 0s 546us/step - loss: 0.1359 - acc: 0.9470 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 13/20\n",
      "868/868 [==============================] - 0s 553us/step - loss: 0.1273 - acc: 0.9447 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 14/20\n",
      "868/868 [==============================] - 0s 533us/step - loss: 0.1254 - acc: 0.9539 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 15/20\n",
      "868/868 [==============================] - 0s 526us/step - loss: 0.1116 - acc: 0.9551 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 16/20\n",
      "868/868 [==============================] - 0s 535us/step - loss: 0.1130 - acc: 0.9539 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 17/20\n",
      "868/868 [==============================] - 0s 525us/step - loss: 0.1078 - acc: 0.9562 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 18/20\n",
      "868/868 [==============================] - 0s 535us/step - loss: 0.1106 - acc: 0.9516 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 19/20\n",
      "868/868 [==============================] - 0s 527us/step - loss: 0.1029 - acc: 0.9597 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 20/20\n",
      "868/868 [==============================] - 0s 541us/step - loss: 0.1009 - acc: 0.9562 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Dataset: Undersampled, Iteration: 3\n",
      "Train on 868 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "868/868 [==============================] - 3s 4ms/step - loss: 0.4583 - acc: 0.8030 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "868/868 [==============================] - 0s 534us/step - loss: 0.2943 - acc: 0.9101 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "868/868 [==============================] - 0s 550us/step - loss: 0.2462 - acc: 0.9113 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "868/868 [==============================] - 0s 529us/step - loss: 0.2078 - acc: 0.9171 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "868/868 [==============================] - 0s 552us/step - loss: 0.1964 - acc: 0.9309 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "868/868 [==============================] - 0s 529us/step - loss: 0.1819 - acc: 0.9297 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "868/868 [==============================] - 0s 541us/step - loss: 0.1611 - acc: 0.9435 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "868/868 [==============================] - 0s 536us/step - loss: 0.1518 - acc: 0.9447 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "868/868 [==============================] - 0s 530us/step - loss: 0.1459 - acc: 0.9355 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "868/868 [==============================] - 0s 537us/step - loss: 0.1371 - acc: 0.9435 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "868/868 [==============================] - 0s 532us/step - loss: 0.1342 - acc: 0.9470 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "868/868 [==============================] - 0s 540us/step - loss: 0.1218 - acc: 0.9482 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "868/868 [==============================] - 0s 530us/step - loss: 0.1250 - acc: 0.9505 - val_loss: 0.0280 - val_acc: 0.9981\n",
      "Epoch 14/20\n",
      "868/868 [==============================] - 0s 542us/step - loss: 0.1152 - acc: 0.9551 - val_loss: 0.0280 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "868/868 [==============================] - 0s 529us/step - loss: 0.1115 - acc: 0.9482 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "868/868 [==============================] - 0s 544us/step - loss: 0.1106 - acc: 0.9562 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "868/868 [==============================] - 0s 532us/step - loss: 0.1105 - acc: 0.9528 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "868/868 [==============================] - 0s 552us/step - loss: 0.1062 - acc: 0.9528 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "868/868 [==============================] - 0s 529us/step - loss: 0.1017 - acc: 0.9620 - val_loss: 0.0290 - val_acc: 0.9980\n",
      "Epoch 20/20\n",
      "868/868 [==============================] - 0s 541us/step - loss: 0.0991 - acc: 0.9585 - val_loss: 0.0280 - val_acc: 0.9981\n",
      "Dataset: Undersampled, Iteration: 4\n",
      "Train on 868 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "868/868 [==============================] - 3s 4ms/step - loss: 0.4273 - acc: 0.7800 - val_loss: 0.0319 - val_acc: 0.9980\n",
      "Epoch 2/20\n",
      "868/868 [==============================] - 0s 542us/step - loss: 0.2901 - acc: 0.8998 - val_loss: 0.0321 - val_acc: 0.9979\n",
      "Epoch 3/20\n",
      "868/868 [==============================] - 0s 551us/step - loss: 0.2371 - acc: 0.9240 - val_loss: 0.0376 - val_acc: 0.9975\n",
      "Epoch 4/20\n",
      "868/868 [==============================] - 1s 584us/step - loss: 0.1998 - acc: 0.9286 - val_loss: 0.0397 - val_acc: 0.9974\n",
      "Epoch 5/20\n",
      "868/868 [==============================] - 0s 566us/step - loss: 0.1820 - acc: 0.9343 - val_loss: 0.0418 - val_acc: 0.9973\n",
      "Epoch 6/20\n",
      "868/868 [==============================] - 0s 542us/step - loss: 0.1688 - acc: 0.9412 - val_loss: 0.0399 - val_acc: 0.9974\n",
      "Epoch 7/20\n",
      "868/868 [==============================] - 0s 552us/step - loss: 0.1604 - acc: 0.9366 - val_loss: 0.0406 - val_acc: 0.9974\n",
      "Epoch 8/20\n",
      "868/868 [==============================] - 0s 557us/step - loss: 0.1514 - acc: 0.9412 - val_loss: 0.0392 - val_acc: 0.9974\n",
      "Epoch 9/20\n",
      "868/868 [==============================] - 1s 584us/step - loss: 0.1441 - acc: 0.9424 - val_loss: 0.0397 - val_acc: 0.9974\n",
      "Epoch 10/20\n",
      "868/868 [==============================] - 0s 539us/step - loss: 0.1409 - acc: 0.9401 - val_loss: 0.0389 - val_acc: 0.9974\n",
      "Epoch 11/20\n",
      "868/868 [==============================] - 0s 552us/step - loss: 0.1464 - acc: 0.9470 - val_loss: 0.0387 - val_acc: 0.9974\n",
      "Epoch 12/20\n",
      "868/868 [==============================] - 0s 545us/step - loss: 0.1249 - acc: 0.9482 - val_loss: 0.0393 - val_acc: 0.9974\n",
      "Epoch 13/20\n",
      "868/868 [==============================] - 0s 555us/step - loss: 0.1384 - acc: 0.9493 - val_loss: 0.0388 - val_acc: 0.9974\n",
      "Epoch 14/20\n",
      "868/868 [==============================] - 0s 541us/step - loss: 0.1278 - acc: 0.9528 - val_loss: 0.0404 - val_acc: 0.9974\n",
      "Epoch 15/20\n",
      "868/868 [==============================] - 0s 550us/step - loss: 0.1193 - acc: 0.9562 - val_loss: 0.0375 - val_acc: 0.9975\n",
      "Epoch 16/20\n",
      "868/868 [==============================] - 0s 549us/step - loss: 0.1170 - acc: 0.9505 - val_loss: 0.0367 - val_acc: 0.9975\n",
      "Epoch 17/20\n",
      "868/868 [==============================] - 0s 542us/step - loss: 0.1056 - acc: 0.9597 - val_loss: 0.0363 - val_acc: 0.9975\n",
      "Epoch 18/20\n",
      "868/868 [==============================] - 0s 548us/step - loss: 0.1151 - acc: 0.9459 - val_loss: 0.0349 - val_acc: 0.9978\n",
      "Epoch 19/20\n",
      "868/868 [==============================] - 1s 585us/step - loss: 0.1028 - acc: 0.9562 - val_loss: 0.0332 - val_acc: 0.9979\n",
      "Epoch 20/20\n",
      "868/868 [==============================] - 0s 551us/step - loss: 0.1089 - acc: 0.9539 - val_loss: 0.0343 - val_acc: 0.9978\n",
      "Dataset: Undersampled, Iteration: 5\n",
      "Train on 868 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "868/868 [==============================] - 3s 4ms/step - loss: 0.5093 - acc: 0.7097 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "868/868 [==============================] - 0s 561us/step - loss: 0.3203 - acc: 0.9032 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "868/868 [==============================] - 0s 548us/step - loss: 0.2534 - acc: 0.9171 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "868/868 [==============================] - 0s 557us/step - loss: 0.2223 - acc: 0.9217 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "868/868 [==============================] - 0s 546us/step - loss: 0.1971 - acc: 0.9240 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "868/868 [==============================] - 0s 548us/step - loss: 0.1742 - acc: 0.9309 - val_loss: 0.0299 - val_acc: 0.9981\n",
      "Epoch 7/20\n",
      "868/868 [==============================] - 0s 556us/step - loss: 0.1613 - acc: 0.9332 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 8/20\n",
      "868/868 [==============================] - 0s 549us/step - loss: 0.1654 - acc: 0.9389 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 9/20\n",
      "868/868 [==============================] - 0s 552us/step - loss: 0.1552 - acc: 0.9435 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 10/20\n",
      "868/868 [==============================] - 0s 573us/step - loss: 0.1441 - acc: 0.9412 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 11/20\n",
      "868/868 [==============================] - 0s 555us/step - loss: 0.1426 - acc: 0.9459 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 12/20\n",
      "868/868 [==============================] - 0s 554us/step - loss: 0.1283 - acc: 0.9551 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 13/20\n",
      "868/868 [==============================] - 0s 555us/step - loss: 0.1325 - acc: 0.9482 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 14/20\n",
      "868/868 [==============================] - 0s 549us/step - loss: 0.1249 - acc: 0.9516 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 15/20\n",
      "868/868 [==============================] - 0s 556us/step - loss: 0.1173 - acc: 0.9562 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 16/20\n",
      "868/868 [==============================] - 0s 548us/step - loss: 0.1148 - acc: 0.9562 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 17/20\n",
      "868/868 [==============================] - 0s 557us/step - loss: 0.1132 - acc: 0.9539 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 18/20\n",
      "868/868 [==============================] - 0s 547us/step - loss: 0.1033 - acc: 0.9585 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 19/20\n",
      "868/868 [==============================] - 0s 556us/step - loss: 0.1085 - acc: 0.9608 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 20/20\n",
      "868/868 [==============================] - 0s 547us/step - loss: 0.0986 - acc: 0.9689 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Dataset: Oversampled, Iteration: 1\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0324 - acc: 0.9887 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0109 - acc: 0.9972 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 50s 97us/step - loss: 0.0087 - acc: 0.9980 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 50s 96us/step - loss: 0.0069 - acc: 0.9984 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 50s 97us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 50s 97us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 50s 97us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 50s 96us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 50s 96us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Oversampled, Iteration: 2\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0357 - acc: 0.9876 - val_loss: 0.0299 - val_acc: 0.9981\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0127 - acc: 0.9968 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0101 - acc: 0.9975 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0090 - acc: 0.9979 - val_loss: 0.0299 - val_acc: 0.9981\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0083 - acc: 0.9981 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0078 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0074 - acc: 0.9984 - val_loss: 0.0299 - val_acc: 0.9981\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0072 - acc: 0.9984 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 51s 99us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0065 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 51s 97us/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0288 - val_acc: 0.9980\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.0283 - val_acc: 0.9980\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Oversampled, Iteration: 3\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0326 - acc: 0.9887 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0121 - acc: 0.9969 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 51s 99us/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0056 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0051 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 51s 99us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0054 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 51s 98us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Oversampled, Iteration: 4\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0333 - acc: 0.9886 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0120 - acc: 0.9968 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0097 - acc: 0.9975 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0088 - acc: 0.9979 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0069 - acc: 0.9984 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0072 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0068 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0059 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 52s 99us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0056 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: Oversampled, Iteration: 5\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 55s 106us/step - loss: 0.0353 - acc: 0.9879 - val_loss: 0.0318 - val_acc: 0.9980\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0117 - acc: 0.9969 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 53s 102us/step - loss: 0.0070 - acc: 0.9984 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 52s 100us/step - loss: 0.0065 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 53s 101us/step - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0061 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 59s 113us/step - loss: 0.0057 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 61s 117us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0052 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: SMOTE, Iteration: 1\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 62s 118us/step - loss: 0.0273 - acc: 0.9907 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 56s 108us/step - loss: 0.0090 - acc: 0.9978 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 58s 111us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 58s 110us/step - loss: 0.0053 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 60s 116us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 57s 110us/step - loss: 0.0044 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 58s 112us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 61s 117us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 59s 112us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 55s 106us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 56s 108us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: SMOTE, Iteration: 2\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 63s 121us/step - loss: 0.0253 - acc: 0.9919 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 59s 114us/step - loss: 0.0093 - acc: 0.9977 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 59s 112us/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0068 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 60s 114us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 57s 110us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 63s 121us/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 58s 111us/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 58s 111us/step - loss: 0.0045 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 58s 111us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0042 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 60s 115us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 59s 112us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 58s 110us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 61s 117us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: SMOTE, Iteration: 3\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 67s 129us/step - loss: 0.0277 - acc: 0.9905 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 64s 122us/step - loss: 0.0090 - acc: 0.9977 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522036/522036 [==============================] - 62s 118us/step - loss: 0.0073 - acc: 0.9983 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 60s 115us/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 61s 116us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 61s 116us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 61s 116us/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 60s 116us/step - loss: 0.0048 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 64s 122us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 60s 116us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 68s 130us/step - loss: 0.0043 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 60s 116us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 60s 115us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 55s 106us/step - loss: 0.0039 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 60s 114us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 61s 116us/step - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 60s 116us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 64s 122us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 64s 122us/step - loss: 0.0038 - acc: 0.9994 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: SMOTE, Iteration: 4\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 64s 123us/step - loss: 0.0276 - acc: 0.9907 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 64s 122us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 62s 118us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 70s 134us/step - loss: 0.0073 - acc: 0.9985 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0067 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 58s 112us/step - loss: 0.0065 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0060 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 56s 107us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 54s 104us/step - loss: 0.0057 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 67s 128us/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 63s 120us/step - loss: 0.0054 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 60s 115us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 60s 115us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 61s 116us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 55s 105us/step - loss: 0.0053 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 59s 113us/step - loss: 0.0052 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 55s 104us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 60s 115us/step - loss: 0.0051 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0050 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Dataset: SMOTE, Iteration: 5\n",
      "Train on 522036 samples, validate on 8087 samples\n",
      "Epoch 1/20\n",
      "522036/522036 [==============================] - 66s 126us/step - loss: 0.0279 - acc: 0.9906 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 2/20\n",
      "522036/522036 [==============================] - 59s 113us/step - loss: 0.0099 - acc: 0.9975 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 3/20\n",
      "522036/522036 [==============================] - 57s 109us/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 4/20\n",
      "522036/522036 [==============================] - 58s 110us/step - loss: 0.0071 - acc: 0.9984 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 5/20\n",
      "522036/522036 [==============================] - 58s 111us/step - loss: 0.0066 - acc: 0.9986 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 6/20\n",
      "522036/522036 [==============================] - 62s 118us/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 7/20\n",
      "522036/522036 [==============================] - 61s 117us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 8/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0055 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 9/20\n",
      "522036/522036 [==============================] - 63s 120us/step - loss: 0.0052 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 10/20\n",
      "522036/522036 [==============================] - 63s 120us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 11/20\n",
      "522036/522036 [==============================] - 61s 116us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 12/20\n",
      "522036/522036 [==============================] - 62s 118us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 13/20\n",
      "522036/522036 [==============================] - 61s 117us/step - loss: 0.0046 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 14/20\n",
      "522036/522036 [==============================] - 59s 114us/step - loss: 0.0045 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 15/20\n",
      "522036/522036 [==============================] - 58s 111us/step - loss: 0.0043 - acc: 0.9991 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 16/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "522036/522036 [==============================] - 70s 134us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 18/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0042 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n",
      "Epoch 19/20\n",
      "522036/522036 [==============================] - 62s 119us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "522036/522036 [==============================] - 69s 132us/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.0279 - val_acc: 0.9983\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "dp = 0.1\n",
    "units = 32\n",
    "batch_size = 16\n",
    "iterations = 5\n",
    "\n",
    "for key in train_datasets_dict:\n",
    "    result = tune_hyperparameters(key, lr, dp, units, batch_size, iterations)\n",
    "    tuning_results = tuning_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Units</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>F1 Avg</th>\n",
       "      <th>F1 Std</th>\n",
       "      <th>Recall Avg</th>\n",
       "      <th>Recall Std</th>\n",
       "      <th>Precision Avg</th>\n",
       "      <th>Precision Std</th>\n",
       "      <th>Accuracy Avg</th>\n",
       "      <th>Accuracy Std</th>\n",
       "      <th>Iterations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.496</td>\n",
       "      <td>4.052456e-01</td>\n",
       "      <td>0.539130</td>\n",
       "      <td>0.440484</td>\n",
       "      <td>0.999013</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.078009</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.960795</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.614738</td>\n",
       "      <td>0.098421</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.472839</td>\n",
       "      <td>0.111801</td>\n",
       "      <td>0.997843</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.511679</td>\n",
       "      <td>0.051370</td>\n",
       "      <td>0.998280</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.419913</td>\n",
       "      <td>0.336</td>\n",
       "      <td>4.115143e-01</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.428661</td>\n",
       "      <td>0.998745</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.064499</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.033435</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>0.949559</td>\n",
       "      <td>0.011972</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.645587</td>\n",
       "      <td>0.061959</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.505760</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.998210</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.710582</td>\n",
       "      <td>0.044788</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.583925</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>0.998675</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.338095</td>\n",
       "      <td>0.414149</td>\n",
       "      <td>0.328</td>\n",
       "      <td>4.019154e-01</td>\n",
       "      <td>0.348913</td>\n",
       "      <td>0.427333</td>\n",
       "      <td>0.998731</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.064615</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.944</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.950786</td>\n",
       "      <td>0.007156</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.653978</td>\n",
       "      <td>0.049611</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.512938</td>\n",
       "      <td>0.061215</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.696742</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.565017</td>\n",
       "      <td>0.042727</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.680952</td>\n",
       "      <td>0.340601</td>\n",
       "      <td>0.664</td>\n",
       "      <td>3.323612e-01</td>\n",
       "      <td>0.698913</td>\n",
       "      <td>0.349463</td>\n",
       "      <td>0.999239</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.059307</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.030639</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.947825</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.701360</td>\n",
       "      <td>0.066504</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.574497</td>\n",
       "      <td>0.085008</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.742228</td>\n",
       "      <td>0.034264</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.628337</td>\n",
       "      <td>0.054662</td>\n",
       "      <td>0.998872</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.855952</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>0.832</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.881522</td>\n",
       "      <td>0.015901</td>\n",
       "      <td>0.999507</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.072299</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.529822e-02</td>\n",
       "      <td>0.037567</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.956509</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.737182</td>\n",
       "      <td>0.033156</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.623206</td>\n",
       "      <td>0.041547</td>\n",
       "      <td>0.998858</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.761806</td>\n",
       "      <td>0.012809</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.650253</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.493224</td>\n",
       "      <td>0.403842</td>\n",
       "      <td>0.464</td>\n",
       "      <td>3.808202e-01</td>\n",
       "      <td>0.526765</td>\n",
       "      <td>0.430536</td>\n",
       "      <td>0.998943</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.077722</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.944</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.960302</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.632591</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.880</td>\n",
       "      <td>2.529822e-02</td>\n",
       "      <td>0.494175</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.998196</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.725293</td>\n",
       "      <td>0.032295</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.606358</td>\n",
       "      <td>0.040140</td>\n",
       "      <td>0.998788</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.822265</td>\n",
       "      <td>0.054001</td>\n",
       "      <td>0.760</td>\n",
       "      <td>1.011929e-01</td>\n",
       "      <td>0.913302</td>\n",
       "      <td>0.053547</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.055810</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.968</td>\n",
       "      <td>3.919184e-02</td>\n",
       "      <td>0.028744</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.941312</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.761677</td>\n",
       "      <td>0.041540</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.655171</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.998985</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.767719</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.872</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.688026</td>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.839062</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>0.776</td>\n",
       "      <td>4.800000e-02</td>\n",
       "      <td>0.915719</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.063186</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.968</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.032697</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.947332</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.835901</td>\n",
       "      <td>0.043442</td>\n",
       "      <td>0.888</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.791503</td>\n",
       "      <td>0.062360</td>\n",
       "      <td>0.999380</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.764508</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.904</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.663702</td>\n",
       "      <td>0.047992</td>\n",
       "      <td>0.999013</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.858341</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.824</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.898047</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.960</td>\n",
       "      <td>3.577709e-02</td>\n",
       "      <td>0.037163</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>0.955635</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.756850</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.085130</td>\n",
       "      <td>0.998929</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.794005</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.904</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.708809</td>\n",
       "      <td>0.051884</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.853867</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.800</td>\n",
       "      <td>5.656854e-02</td>\n",
       "      <td>0.920950</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.104473</td>\n",
       "      <td>0.009737</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.055167</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.969747</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.855007</td>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.896</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.817725</td>\n",
       "      <td>0.021071</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.825400</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.872</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>0.023058</td>\n",
       "      <td>0.999352</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.867076</td>\n",
       "      <td>0.038706</td>\n",
       "      <td>0.816</td>\n",
       "      <td>6.499231e-02</td>\n",
       "      <td>0.930007</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.960</td>\n",
       "      <td>2.529822e-02</td>\n",
       "      <td>0.052454</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.968873</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.789848</td>\n",
       "      <td>0.050029</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.700934</td>\n",
       "      <td>0.076833</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.825312</td>\n",
       "      <td>0.017780</td>\n",
       "      <td>0.888</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.771883</td>\n",
       "      <td>0.026160</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.784</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.909545</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.104051</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.952</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.055072</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>0.970748</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.796174</td>\n",
       "      <td>0.016187</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.702119</td>\n",
       "      <td>0.025315</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.795802</td>\n",
       "      <td>0.036520</td>\n",
       "      <td>0.888</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.722007</td>\n",
       "      <td>0.040388</td>\n",
       "      <td>0.999196</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.035797</td>\n",
       "      <td>0.744</td>\n",
       "      <td>5.425864e-02</td>\n",
       "      <td>0.943337</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.135940</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>0.920</td>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>0.073520</td>\n",
       "      <td>0.011272</td>\n",
       "      <td>0.978910</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.788711</td>\n",
       "      <td>0.028393</td>\n",
       "      <td>0.904</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.701665</td>\n",
       "      <td>0.048932</td>\n",
       "      <td>0.999140</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.798830</td>\n",
       "      <td>0.029344</td>\n",
       "      <td>0.872</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.739554</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.848659</td>\n",
       "      <td>0.037982</td>\n",
       "      <td>0.784</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.925108</td>\n",
       "      <td>0.046817</td>\n",
       "      <td>0.999507</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.118274</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.959592e-02</td>\n",
       "      <td>0.063165</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.975174</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.051752</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.600000e-02</td>\n",
       "      <td>0.732836</td>\n",
       "      <td>0.088523</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.784335</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>0.872</td>\n",
       "      <td>4.664762e-02</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>0.999154</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unsampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.847587</td>\n",
       "      <td>0.054736</td>\n",
       "      <td>0.776</td>\n",
       "      <td>9.666437e-02</td>\n",
       "      <td>0.946894</td>\n",
       "      <td>0.043532</td>\n",
       "      <td>0.999521</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Undersampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.095007</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>0.968</td>\n",
       "      <td>2.993326e-02</td>\n",
       "      <td>0.049994</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.967083</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oversampled</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.787298</td>\n",
       "      <td>0.040833</td>\n",
       "      <td>0.904</td>\n",
       "      <td>3.200000e-02</td>\n",
       "      <td>0.702016</td>\n",
       "      <td>0.070453</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMOTE</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0.792552</td>\n",
       "      <td>0.039712</td>\n",
       "      <td>0.880</td>\n",
       "      <td>4.381780e-02</td>\n",
       "      <td>0.724465</td>\n",
       "      <td>0.064169</td>\n",
       "      <td>0.999182</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dataset  Learning Rate  Dropout  Units  Batch Size    F1 Avg  \\\n",
       "0      Unsampled         0.0100      0.1     64          32  0.516667   \n",
       "1   Undersampled         0.0100      0.1     64          32  0.078009   \n",
       "2    Oversampled         0.0100      0.1     64          32  0.614738   \n",
       "3          SMOTE         0.0100      0.1     64          32  0.656100   \n",
       "4      Unsampled         0.0100      0.2     64          32  0.342857   \n",
       "5   Undersampled         0.0100      0.2     64          32  0.064499   \n",
       "6    Oversampled         0.0100      0.2     64          32  0.645587   \n",
       "7          SMOTE         0.0100      0.2     64          32  0.710582   \n",
       "8      Unsampled         0.0100      0.3     64          32  0.338095   \n",
       "9   Undersampled         0.0100      0.3     64          32  0.064615   \n",
       "10   Oversampled         0.0100      0.3     64          32  0.653978   \n",
       "11         SMOTE         0.0100      0.3     64          32  0.696742   \n",
       "12     Unsampled         0.0100      0.1     32          32  0.680952   \n",
       "13  Undersampled         0.0100      0.1     32          32  0.059307   \n",
       "14   Oversampled         0.0100      0.1     32          32  0.701360   \n",
       "15         SMOTE         0.0100      0.1     32          32  0.742228   \n",
       "16     Unsampled         0.0050      0.2     32          32  0.855952   \n",
       "17  Undersampled         0.0050      0.2     32          32  0.072299   \n",
       "18   Oversampled         0.0050      0.2     32          32  0.737182   \n",
       "19         SMOTE         0.0050      0.2     32          32  0.761806   \n",
       "20     Unsampled         0.0100      0.2     32          32  0.493224   \n",
       "21  Undersampled         0.0100      0.2     32          32  0.077722   \n",
       "22   Oversampled         0.0100      0.2     32          32  0.632591   \n",
       "23         SMOTE         0.0100      0.2     32          32  0.725293   \n",
       "24     Unsampled         0.0100      0.0     32          32  0.822265   \n",
       "25  Undersampled         0.0100      0.0     32          32  0.055810   \n",
       "26   Oversampled         0.0100      0.0     32          32  0.761677   \n",
       "27         SMOTE         0.0100      0.0     32          32  0.767719   \n",
       "28     Unsampled         0.0050      0.0     32          32  0.839062   \n",
       "29  Undersampled         0.0050      0.0     32          32  0.063186   \n",
       "30   Oversampled         0.0050      0.0     32          32  0.835901   \n",
       "31         SMOTE         0.0050      0.0     32          32  0.764508   \n",
       "32     Unsampled         0.0050      0.1     32          32  0.858341   \n",
       "33  Undersampled         0.0050      0.1     32          32  0.071521   \n",
       "34   Oversampled         0.0050      0.1     32          32  0.756850   \n",
       "35         SMOTE         0.0050      0.1     32          32  0.794005   \n",
       "36     Unsampled         0.0010      0.0     32          32  0.853867   \n",
       "37  Undersampled         0.0010      0.0     32          32  0.104473   \n",
       "38   Oversampled         0.0010      0.0     32          32  0.855007   \n",
       "39         SMOTE         0.0010      0.0     32          32  0.825400   \n",
       "40     Unsampled         0.0010      0.1     32          32  0.867076   \n",
       "41  Undersampled         0.0010      0.1     32          32  0.099392   \n",
       "42   Oversampled         0.0010      0.1     32          32  0.789848   \n",
       "43         SMOTE         0.0010      0.1     32          32  0.825312   \n",
       "44     Unsampled         0.0010      0.2     32          32  0.841121   \n",
       "45  Undersampled         0.0010      0.2     32          32  0.104051   \n",
       "46   Oversampled         0.0010      0.2     32          32  0.796174   \n",
       "47         SMOTE         0.0010      0.2     32          32  0.795802   \n",
       "48     Unsampled         0.0005      0.2     32          32  0.829757   \n",
       "49  Undersampled         0.0005      0.2     32          32  0.135940   \n",
       "50   Oversampled         0.0005      0.2     32          32  0.788711   \n",
       "51         SMOTE         0.0005      0.2     32          32  0.798830   \n",
       "0      Unsampled         0.0005      0.1     32          32  0.848659   \n",
       "0   Undersampled         0.0005      0.1     32          32  0.118274   \n",
       "0    Oversampled         0.0005      0.1     32          32  0.809278   \n",
       "0          SMOTE         0.0005      0.1     32          32  0.784335   \n",
       "0      Unsampled         0.0010      0.1     32          16  0.847587   \n",
       "0   Undersampled         0.0010      0.1     32          16  0.095007   \n",
       "0    Oversampled         0.0010      0.1     32          16  0.787298   \n",
       "0          SMOTE         0.0010      0.1     32          16  0.792552   \n",
       "\n",
       "      F1 Std  Recall Avg    Recall Std  Precision Avg  Precision Std  \\\n",
       "0   0.422131       0.496  4.052456e-01       0.539130       0.440484   \n",
       "1   0.009770       0.928  1.600000e-02       0.040738       0.005308   \n",
       "2   0.098421       0.912  1.600000e-02       0.472839       0.111801   \n",
       "3   0.042169       0.920  1.110223e-16       0.511679       0.051370   \n",
       "4   0.419913       0.336  4.115143e-01       0.350000       0.428661   \n",
       "5   0.012134       0.944  1.959592e-02       0.033435       0.006487   \n",
       "6   0.061959       0.904  1.959592e-02       0.505760       0.075151   \n",
       "7   0.044788       0.912  1.600000e-02       0.583925       0.056182   \n",
       "8   0.414149       0.328  4.019154e-01       0.348913       0.427333   \n",
       "9   0.009789       0.944  3.200000e-02       0.033478       0.005242   \n",
       "10  0.049611       0.912  1.600000e-02       0.512938       0.061215   \n",
       "11  0.033807       0.912  1.600000e-02       0.565017       0.042727   \n",
       "12  0.340601       0.664  3.323612e-01       0.698913       0.349463   \n",
       "13  0.004300       0.928  1.600000e-02       0.030639       0.002307   \n",
       "14  0.066504       0.912  1.600000e-02       0.574497       0.085008   \n",
       "15  0.034264       0.912  1.600000e-02       0.628337       0.054662   \n",
       "16  0.013257       0.832  1.600000e-02       0.881522       0.015901   \n",
       "17  0.003442       0.960  2.529822e-02       0.037567       0.001841   \n",
       "18  0.033156       0.904  1.959592e-02       0.623206       0.041547   \n",
       "19  0.012809       0.920  1.110223e-16       0.650253       0.018608   \n",
       "20  0.403842       0.464  3.808202e-01       0.526765       0.430536   \n",
       "21  0.005147       0.944  1.959592e-02       0.040541       0.002829   \n",
       "22  0.022157       0.880  2.529822e-02       0.494175       0.023940   \n",
       "23  0.032295       0.904  1.959592e-02       0.606358       0.040140   \n",
       "24  0.054001       0.760  1.011929e-01       0.913302       0.053547   \n",
       "25  0.007100       0.968  3.919184e-02       0.028744       0.003732   \n",
       "26  0.041540       0.912  1.600000e-02       0.655171       0.053576   \n",
       "27  0.056569       0.872  2.993326e-02       0.688026       0.073092   \n",
       "28  0.026614       0.776  4.800000e-02       0.915719       0.016394   \n",
       "29  0.010742       0.968  1.600000e-02       0.032697       0.005726   \n",
       "30  0.043442       0.888  2.993326e-02       0.791503       0.062360   \n",
       "31  0.036624       0.904  3.200000e-02       0.663702       0.047992   \n",
       "32  0.014841       0.824  3.200000e-02       0.898047       0.036887   \n",
       "33  0.006630       0.960  3.577709e-02       0.037163       0.003612   \n",
       "34  0.060802       0.920  1.110223e-16       0.647619       0.085130   \n",
       "35  0.042254       0.904  3.200000e-02       0.708809       0.051884   \n",
       "36  0.021458       0.800  5.656854e-02       0.920950       0.035547   \n",
       "37  0.009737       0.992  1.600000e-02       0.055167       0.005418   \n",
       "38  0.019059       0.896  1.959592e-02       0.817725       0.021071   \n",
       "39  0.016656       0.872  4.664762e-02       0.785689       0.023058   \n",
       "40  0.038706       0.816  6.499231e-02       0.930007       0.044900   \n",
       "41  0.012528       0.960  2.529822e-02       0.052454       0.006989   \n",
       "42  0.050029       0.912  1.600000e-02       0.700934       0.076833   \n",
       "43  0.017780       0.888  2.993326e-02       0.771883       0.026160   \n",
       "44  0.016977       0.784  3.200000e-02       0.909545       0.036261   \n",
       "45  0.011450       0.952  2.993326e-02       0.055072       0.006395   \n",
       "46  0.016187       0.920  1.110223e-16       0.702119       0.025315   \n",
       "47  0.036520       0.888  4.664762e-02       0.722007       0.040388   \n",
       "48  0.035797       0.744  5.425864e-02       0.943337       0.053493   \n",
       "49  0.019270       0.920  1.110223e-16       0.073520       0.011272   \n",
       "50  0.028393       0.904  1.959592e-02       0.701665       0.048932   \n",
       "51  0.029344       0.872  4.664762e-02       0.739554       0.042900   \n",
       "0   0.037982       0.784  3.200000e-02       0.925108       0.046817   \n",
       "0   0.011065       0.936  1.959592e-02       0.063165       0.006301   \n",
       "0   0.051752       0.912  1.600000e-02       0.732836       0.088523   \n",
       "0   0.036037       0.872  4.664762e-02       0.713900       0.040837   \n",
       "0   0.054736       0.776  9.666437e-02       0.946894       0.043532   \n",
       "0   0.010841       0.968  2.993326e-02       0.049994       0.006048   \n",
       "0   0.040833       0.904  3.200000e-02       0.702016       0.070453   \n",
       "0   0.039712       0.880  4.381780e-02       0.724465       0.064169   \n",
       "\n",
       "    Accuracy Avg  Accuracy Std  Iterations  \n",
       "0       0.999013      0.000635           5  \n",
       "1       0.960795      0.004513           5  \n",
       "2       0.997843      0.000893           5  \n",
       "3       0.998280      0.000317           5  \n",
       "4       0.998745      0.000622           5  \n",
       "5       0.949559      0.011972           5  \n",
       "6       0.998210      0.000456           5  \n",
       "7       0.998675      0.000283           5  \n",
       "8       0.998731      0.000605           5  \n",
       "9       0.950786      0.007156           5  \n",
       "10      0.998266      0.000415           5  \n",
       "11      0.998590      0.000232           5  \n",
       "12      0.999239      0.000501           5  \n",
       "13      0.947825      0.004191           5  \n",
       "14      0.998590      0.000452           5  \n",
       "15      0.998872      0.000214           5  \n",
       "16      0.999507      0.000045           5  \n",
       "17      0.956509      0.001917           5  \n",
       "18      0.998858      0.000181           5  \n",
       "19      0.998985      0.000072           5  \n",
       "20      0.998943      0.000583           5  \n",
       "21      0.960302      0.003327           5  \n",
       "22      0.998196      0.000145           5  \n",
       "23      0.998788      0.000175           5  \n",
       "24      0.999436      0.000118           5  \n",
       "25      0.941312      0.007919           5  \n",
       "26      0.998985      0.000221           5  \n",
       "27      0.999055      0.000288           5  \n",
       "28      0.999478      0.000072           5  \n",
       "29      0.947332      0.012448           5  \n",
       "30      0.999380      0.000186           5  \n",
       "31      0.999013      0.000178           5  \n",
       "32      0.999521      0.000053           5  \n",
       "33      0.955635      0.004936           5  \n",
       "34      0.998929      0.000371           5  \n",
       "35      0.999168      0.000191           5  \n",
       "36      0.999521      0.000053           5  \n",
       "37      0.969747      0.003124           5  \n",
       "38      0.999464      0.000072           5  \n",
       "39      0.999352      0.000053           5  \n",
       "40      0.999563      0.000121           5  \n",
       "41      0.968873      0.003797           5  \n",
       "42      0.999126      0.000284           5  \n",
       "43      0.999337      0.000072           5  \n",
       "44      0.999478      0.000056           5  \n",
       "45      0.970748      0.003463           5  \n",
       "46      0.999168      0.000082           5  \n",
       "47      0.999196      0.000145           5  \n",
       "48      0.999464      0.000105           5  \n",
       "49      0.978910      0.003425           5  \n",
       "50      0.999140      0.000157           5  \n",
       "51      0.999225      0.000126           5  \n",
       "0       0.999507      0.000126           5  \n",
       "0       0.975174      0.002551           5  \n",
       "0       0.999225      0.000260           5  \n",
       "0       0.999154      0.000148           5  \n",
       "0       0.999521      0.000137           5  \n",
       "0       0.967083      0.003828           5  \n",
       "0       0.999126      0.000230           5  \n",
       "0       0.999182      0.000176           5  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results.to_csv(tuning_path, index = False, columns = [\"Dataset\", \"Learning Rate\", \"Dropout\", \"Units\", \"Batch Size\", \"F1 Avg\", \"F1 Std\", \"Recall Avg\", \"Recall Std\", \"Precision Avg\", \"Precision Std\", \"Accuracy Avg\", \"Accuracy Std\", \"Iterations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting models\n",
    "model_dict = {}\n",
    "hist_dict = {}\n",
    "for key in train_datasets_dict:\n",
    "    model, hist = train_model(train_features_dict[key].values, train_labels_dict[key].values, val_features.values, val_labels.values)\n",
    "    model_dict[key] = model\n",
    "    hist_dict[key] = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting models\n",
    "for key, val in hist_dict.items():\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.title(key)\n",
    "    plt.plot(hist_dict[key].history[\"loss\"], label = \"Training loss\")\n",
    "    plt.plot(hist_dict[key].history[\"val_loss\"], label = \"Validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Dataset Fraud Count\\n{}\".format(test_labels[\"label\"].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "for key, val in hist_dict.items():\n",
    "    predictions = model_dict[key].predict_classes(test_features)\n",
    "    acc = accuracy_score(test_labels, predictions)\n",
    "    prec = precision_score(test_labels, predictions)\n",
    "    rec = recall_score(test_labels, predictions)\n",
    "    f1 = f1_score(test_labels, predictions)\n",
    "    c_matrix = confusion_matrix(test_labels, predictions)\n",
    "    \n",
    "    print(\"{} Metrics Report\".format(key))\n",
    "    metrics_dict = {\"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1}\n",
    "    for name, metric in metrics_dict.items():\n",
    "        print(\"{}: {}\".format(name, round(metric, 4)))\n",
    "    print(\"True Negative: {}\".format(c_matrix.ravel()[0]))\n",
    "    print(\"False Positive: {}\".format(c_matrix.ravel()[1]))\n",
    "    print(\"False Negative: {}\".format(c_matrix.ravel()[2]))\n",
    "    print(\"True Positive: {}\".format(c_matrix.ravel()[3]))\n",
    "    print(c_matrix)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**lr = 0.05, epochs = 50\n",
    "Test Dataset Fraud Count\n",
    "0    14162\n",
    "1       25\n",
    "Name: label, dtype: int64**\n",
    "\n",
    "Unsampled Metrics Report\n",
    "- Accuracy: 0.9982\n",
    "Precision: 0.0\n",
    "Recall: 0.0\n",
    "F1: 0.0\n",
    "- True Negative: 14162\n",
    "False Positive: 0\n",
    "False Negative: 25\n",
    "True Positive: 0\n",
    "- [[14162     0]\n",
    " [   25     0]]\n",
    "\n",
    "Undersampled Metrics Report\n",
    "- Accuracy: 0.9394\n",
    "Precision: 0.025\n",
    "Recall: 0.88\n",
    "F1: 0.0487\n",
    "- True Negative: 13305\n",
    "False Positive: 857\n",
    "False Negative: 3\n",
    "True Positive: 22\n",
    "- [[13305   857]\n",
    " [    3    22]]\n",
    "\n",
    "Oversampled Metrics Report\n",
    "- Accuracy: 0.9991\n",
    "Precision: 0.75\n",
    "Recall: 0.72\n",
    "F1: 0.7347\n",
    "- True Negative: 14156\n",
    "False Positive: 6\n",
    "False Negative: 7\n",
    "True Positive: 18\n",
    "- [[14156     6]\n",
    " [    7    18]]\n",
    "\n",
    "SMOTE Metrics Report\n",
    "- Accuracy: 0.9807\n",
    "Precision: 0.0751\n",
    "Recall: 0.88\n",
    "F1: 0.1384\n",
    "- True Negative: 13891\n",
    "False Positive: 271\n",
    "False Negative: 3\n",
    "True Positive: 22\n",
    "- [[13891   271]\n",
    " [    3    22]]\n",
    " \n",
    "**lr = 0.05, epochs = 20\n",
    "Test Dataset Fraud Count\n",
    "0    14162\n",
    "1       25\n",
    "Name: label, dtype: int64**\n",
    "\n",
    "Unsampled Metrics Report\n",
    "- Accuracy: 0.9982\n",
    "Precision: 0.0\n",
    "Recall: 0.0\n",
    "F1: 0.0\n",
    "- True Negative: 14162\n",
    "False Positive: 0\n",
    "False Negative: 25\n",
    "True Positive: 0\n",
    "- [[14162     0]\n",
    " [   25     0]]\n",
    "\n",
    "Undersampled Metrics Report\n",
    "- Accuracy: 0.9385\n",
    "Precision: 0.0236\n",
    "Recall: 0.84\n",
    "F1: 0.0459\n",
    "- True Negative: 13293\n",
    "False Positive: 869\n",
    "False Negative: 4\n",
    "True Positive: 21\n",
    "- [[13293   869]\n",
    " [    4    21]]\n",
    "\n",
    "Oversampled Metrics Report\n",
    "- Accuracy: 0.9993\n",
    "Precision: 0.8261\n",
    "Recall: 0.76\n",
    "F1: 0.7917\n",
    "- True Negative: 14158\n",
    "False Positive: 4\n",
    "False Negative: 6\n",
    "True Positive: 19\n",
    "- [[14158     4]\n",
    " [    6    19]]\n",
    "\n",
    "SMOTE Metrics Report\n",
    "- Accuracy: 0.9867\n",
    "Precision: 0.1063\n",
    "Recall: 0.88\n",
    "F1: 0.1897\n",
    "- True Negative: 13977\n",
    "False Positive: 185\n",
    "False Negative: 3\n",
    "True Positive: 22\n",
    "- [[13977   185]\n",
    " [    3    22]]\n",
    "\n",
    "**lr = 0.05, epochs = 10\n",
    "Test Dataset Fraud Count\n",
    "0    14162\n",
    "1       25\n",
    "Name: label, dtype: int64**\n",
    "\n",
    "Unsampled Metrics Report\n",
    "Accuracy: 0.9982\n",
    "Precision: 0.0\n",
    "Recall: 0.0\n",
    "F1: 0.0\n",
    "True Negative: 14162\n",
    "False Positive: 0\n",
    "False Negative: 25\n",
    "True Positive: 0\n",
    "[[14162     0]\n",
    " [   25     0]]\n",
    "\n",
    "Undersampled Metrics Report\n",
    "- Accuracy: 0.9438\n",
    "Precision: 0.027\n",
    "Recall: 0.88\n",
    "F1: 0.0523\n",
    "- True Negative: 13368\n",
    "False Positive: 794\n",
    "False Negative: 3\n",
    "True Positive: 22\n",
    "- [[13368   794]\n",
    " [    3    22]]\n",
    "\n",
    "Oversampled Metrics Report\n",
    "- Accuracy: 0.9982\n",
    "Precision: 0.0\n",
    "Recall: 0.0\n",
    "F1: 0.0\n",
    "- True Negative: 14162\n",
    "False Positive: 0\n",
    "False Negative: 25\n",
    "True Positive: 0\n",
    "- [[14162     0]\n",
    " [   25     0]]\n",
    "\n",
    "SMOTE Metrics Report\n",
    "- Accuracy: 0.9937\n",
    "Precision: 0.1923\n",
    "Recall: 0.8\n",
    "F1: 0.3101\n",
    "- True Negative: 14078\n",
    "False Positive: 84\n",
    "False Negative: 5\n",
    "True Positive: 20\n",
    "- [[14078    84]\n",
    " [    5    20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision = Actual / Predicted ( Wrong Predictions )\n",
    "    - TP / (TP + FP)\n",
    "- Recall = Predicted / Actual ( Missed Predictions )\n",
    "    - TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
